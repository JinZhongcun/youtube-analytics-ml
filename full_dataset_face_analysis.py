#!/usr/bin/env python3
"""
å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ(6,062ä»¶)ã§ã®é¡”æ¤œå‡ºç‰¹å¾´é‡åˆ†æ - ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãªã—
ä½•æ™‚é–“ã§ã‚‚ã‹ã‘ã¦å¾¹åº•çš„ã«å®Œå…¨åˆ†æ
"""
import pandas as pd
import numpy as np
import cv2
import os
import json
from sklearn.model_selection import cross_val_score, KFold
import lightgbm as lgb
import warnings
warnings.filterwarnings('ignore')

print("="*60)
print("å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ6,062ä»¶ï¼‰ã§ã®é¡”æ¤œå‡ºç‰¹å¾´é‡å®Œå…¨åˆ†æ")
print("="*60)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv('youtube_top_new_complete.csv')
print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(df)}ä»¶")

# ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã®å­˜åœ¨ç¢ºèª
thumbnails_dir = 'thumbnails'
df['thumbnail_exists'] = df['video_id'].apply(
    lambda x: os.path.exists(os.path.join(thumbnails_dir, f'{x}.jpg'))
)
print(f"ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒãŒå­˜åœ¨: {df['thumbnail_exists'].sum()}/{len(df)}ä»¶")

# ç”»åƒãŒå­˜åœ¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãªã—ï¼‰
df_with_images = df[df['thumbnail_exists']].copy()
print(f"åˆ†æå¯¾è±¡: {len(df_with_images)}ä»¶ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰")

def detect_face(video_id):
    """OpenCV Haar Cascadeã§é¡”æ¤œå‡º"""
    img_path = os.path.join(thumbnails_dir, f'{video_id}.jpg')
    
    try:
        img = cv2.imread(img_path)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # é¡”æ¤œå‡º
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)
        
        return int(len(faces) > 0)
    except Exception as e:
        return 0

# å…¨ãƒ‡ãƒ¼ã‚¿ã§é¡”æ¤œå‡ºå®Ÿè¡Œï¼ˆä½•æ™‚é–“ã§ã‚‚ã‹ã‘ã‚‹ï¼‰
print(f"\n=== å…¨ãƒ‡ãƒ¼ã‚¿é¡”æ¤œå‡ºå®Ÿè¡Œä¸­ï¼ˆ{len(df_with_images)}ä»¶ï¼‰===")
print("âš ï¸  æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ãŒå…¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã—ã¾ã™...")

face_results = []
total_count = len(df_with_images)

for i, video_id in enumerate(df_with_images['video_id']):
    if i % 250 == 0:  # ã‚ˆã‚Šé »ç¹ã«é€²æ—è¡¨ç¤º
        print(f"  {i}/{total_count}ä»¶å‡¦ç†æ¸ˆã¿ ({i/total_count*100:.1f}%) - ç¶™ç¶šä¸­...")
    
    has_face = detect_face(video_id)
    face_results.append(has_face)

df_with_images['has_face'] = face_results

# åŸºæœ¬çµ±è¨ˆ
face_count = sum(face_results)
no_face_count = len(face_results) - face_count
print(f"\né¡”æ¤œå‡ºçµæœ:")
print(f"  é¡”ã‚ã‚Š: {face_count}ä»¶ ({face_count/len(face_results)*100:.1f}%)")
print(f"  é¡”ãªã—: {no_face_count}ä»¶ ({no_face_count/len(face_results)*100:.1f}%)")

# æ™‚é–“é–¢é€£ã®ç‰¹å¾´é‡ä½œæˆ
df_with_images['published_at'] = pd.to_datetime(df_with_images['published_at']).dt.tz_localize(None)
df_with_images['hour_published'] = df_with_images['published_at'].dt.hour
df_with_images['days_since_publish'] = (pd.Timestamp.now() - df_with_images['published_at']).dt.days

# ãƒ­ã‚°å¤‰æ›
df_with_images['log_views'] = np.log10(df_with_images['views'] + 1)
df_with_images['log_subscribers'] = np.log1p(df_with_images['subscribers'])

# åŸºæœ¬ç‰¹å¾´é‡ï¼ˆé¡”æ¤œå‡ºãªã—ï¼‰
basic_features = [
    'video_duration', 'tags_count', 'description_length',
    'object_complexity', 'element_complexity', 'brightness', 'colorfulness',
    'subscribers', 'log_subscribers', 'hour_published', 'days_since_publish'
]

# é¡”æ¤œå‡ºã‚’å«ã‚€ç‰¹å¾´é‡
face_features = basic_features + ['has_face']

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X_basic = df_with_images[basic_features].fillna(0)
X_face = df_with_images[face_features].fillna(0)
y = df_with_images['log_views']

print(f"\nç‰¹å¾´é‡æº–å‚™å®Œäº†:")
print(f"  åŸºæœ¬ç‰¹å¾´é‡: {len(basic_features)}å€‹")
print(f"  é¡”æ¤œå‡ºå«ã‚€: {len(face_features)}å€‹")
print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X_basic)}ä»¶")

# äº¤å·®æ¤œè¨¼è¨­å®š
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

print(f"\n=== ãƒ¢ãƒ‡ãƒ«1: åŸºæœ¬ç‰¹å¾´é‡ã®ã¿ï¼ˆ{len(basic_features)}ç‰¹å¾´é‡ï¼‰===")
lgb_basic = lgb.LGBMRegressor(
    num_leaves=31, max_depth=6, min_child_samples=30,
    lambda_l2=0.1, feature_fraction=0.8, bagging_fraction=0.8,
    learning_rate=0.05, n_estimators=200, random_state=42, verbosity=-1
)

print("äº¤å·®æ¤œè¨¼å®Ÿè¡Œä¸­...")
cv_scores_basic = cross_val_score(lgb_basic, X_basic, y, cv=kfold, scoring='r2')
print(f"CV RÂ²: {cv_scores_basic.mean():.4f} Â± {cv_scores_basic.std():.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦å–å¾—
print("ç‰¹å¾´é‡é‡è¦åº¦è¨ˆç®—ä¸­...")
lgb_basic.fit(X_basic, y)
importance_basic = pd.DataFrame({
    'feature': basic_features,
    'importance': lgb_basic.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\nåŸºæœ¬ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆTop 10ï¼‰:")
total_importance_basic = importance_basic['importance'].sum()
for _, row in importance_basic.head(10).iterrows():
    pct = (row['importance'] / total_importance_basic) * 100
    print(f"  {row['feature']:<20} {row['importance']:>6.0f} ({pct:>5.1f}%)")

print(f"\n=== ãƒ¢ãƒ‡ãƒ«2: é¡”æ¤œå‡ºç‰¹å¾´é‡å«ã‚€ï¼ˆ{len(face_features)}ç‰¹å¾´é‡ï¼‰===")
lgb_face = lgb.LGBMRegressor(
    num_leaves=31, max_depth=6, min_child_samples=30,
    lambda_l2=0.1, feature_fraction=0.8, bagging_fraction=0.8,
    learning_rate=0.05, n_estimators=200, random_state=42, verbosity=-1
)

print("äº¤å·®æ¤œè¨¼å®Ÿè¡Œä¸­...")
cv_scores_face = cross_val_score(lgb_face, X_face, y, cv=kfold, scoring='r2')
print(f"CV RÂ²: {cv_scores_face.mean():.4f} Â± {cv_scores_face.std():.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦å–å¾—
print("ç‰¹å¾´é‡é‡è¦åº¦è¨ˆç®—ä¸­...")
lgb_face.fit(X_face, y)
importance_face = pd.DataFrame({
    'feature': face_features,
    'importance': lgb_face.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\né¡”æ¤œå‡ºå«ã‚€ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆå…¨12ç‰¹å¾´é‡ï¼‰:")
total_importance_face = importance_face['importance'].sum()
for _, row in importance_face.iterrows():
    pct = (row['importance'] / total_importance_face) * 100
    marker = " â† ğŸ‘¤é¡”æ¤œå‡º" if row['feature'] == 'has_face' else ""
    print(f"  {row['feature']:<20} {row['importance']:>6.0f} ({pct:>5.1f}%){marker}")

# æ€§èƒ½å·®åˆ†æ
performance_diff = cv_scores_face.mean() - cv_scores_basic.mean()
print(f"\n=== æ€§èƒ½æ¯”è¼ƒï¼ˆå…¨{len(df_with_images)}ä»¶ãƒ‡ãƒ¼ã‚¿ï¼‰===")
print(f"åŸºæœ¬ãƒ¢ãƒ‡ãƒ«:     RÂ² = {cv_scores_basic.mean():.4f} Â± {cv_scores_basic.std():.4f}")
print(f"é¡”æ¤œå‡ºå«ã‚€:     RÂ² = {cv_scores_face.mean():.4f} Â± {cv_scores_face.std():.4f}")
print(f"æ€§èƒ½å·®:         {performance_diff:+.4f}")
print(f"æ”¹å–„ç‡:         {(performance_diff/cv_scores_basic.mean())*100:+.2f}%")

# é¡”æ¤œå‡ºç‰¹å¾´é‡ã®é †ä½
has_face_row = importance_face[importance_face['feature'] == 'has_face']
has_face_rank = has_face_row.index[0] + 1
has_face_importance = has_face_row['importance'].iloc[0]
has_face_pct = (has_face_importance / total_importance_face) * 100

print(f"\n=== é¡”æ¤œå‡ºç‰¹å¾´é‡ã®è©³ç´°åˆ†æ ===")
print(f"has_faceé †ä½:   {has_face_rank}ä½ / {len(face_features)}ç‰¹å¾´é‡")
print(f"é‡è¦åº¦:         {has_face_importance:.0f} ({has_face_pct:.2f}%)")
print(f"ãƒ‡ãƒ¼ã‚¿è¦æ¨¡:     {len(df_with_images):,}ä»¶ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ï¼‰")

# çµæœä¿å­˜
results = {
    'analysis_info': {
        'dataset_size': len(df),
        'with_thumbnails': len(df_with_images),
        'analysis_type': 'FULL_DATASET_NO_SAMPLING',
        'face_count': int(face_count),
        'no_face_count': int(no_face_count),
        'face_ratio': float(face_count/len(face_results))
    },
    'model_performance': {
        'basic_model': {
            'features_count': len(basic_features),
            'cv_r2_mean': float(cv_scores_basic.mean()),
            'cv_r2_std': float(cv_scores_basic.std()),
            'features': basic_features
        },
        'face_model': {
            'features_count': len(face_features),
            'cv_r2_mean': float(cv_scores_face.mean()),
            'cv_r2_std': float(cv_scores_face.std()),
            'features': face_features
        },
        'comparison': {
            'performance_diff': float(performance_diff),
            'improvement_rate_pct': float((performance_diff/cv_scores_basic.mean())*100),
            'is_improvement': performance_diff > 0
        }
    },
    'feature_importance': {
        'basic_model_top10': [
            {
                'feature': row['feature'],
                'importance': float(row['importance']),
                'percentage': float((row['importance'] / total_importance_basic) * 100)
            }
            for _, row in importance_basic.head(10).iterrows()
        ],
        'face_model_all': [
            {
                'feature': row['feature'],
                'importance': float(row['importance']),
                'percentage': float((row['importance'] / total_importance_face) * 100),
                'rank': int(i + 1)
            }
            for i, (_, row) in enumerate(importance_face.iterrows())
        ]
    },
    'face_feature_analysis': {
        'rank': int(has_face_rank),
        'importance': float(has_face_importance),
        'percentage': float(has_face_pct),
        'is_effective': performance_diff > 0,
        'conclusion': 'EFFECTIVE' if performance_diff > 0 else 'INEFFECTIVE'
    }
}

# è©³ç´°ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜
df_with_images.to_csv('youtube_complete_with_faces_full.csv', index=False)

with open('full_dataset_face_analysis_results.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"\n=== æœ€çµ‚çµè«–ï¼ˆå…¨{len(df_with_images):,}ä»¶ãƒ‡ãƒ¼ã‚¿ï¼‰===")
if performance_diff > 0:
    print(f"âœ… é¡”æ¤œå‡ºç‰¹å¾´é‡ã¯æ€§èƒ½ã‚’ {performance_diff:.4f} æ”¹å–„")
    print(f"âœ… {has_face_rank}ä½ã®é‡è¦åº¦ ({has_face_pct:.2f}%)")
    conclusion = "æœ‰åŠ¹"
else:
    print(f"âŒ é¡”æ¤œå‡ºç‰¹å¾´é‡ã¯æ€§èƒ½ã‚’ {performance_diff:.4f} æ‚ªåŒ–")
    print(f"âŒ {has_face_rank}ä½ã®é‡è¦åº¦ ({has_face_pct:.2f}%)")
    conclusion = "ç„¡åŠ¹"

print(f"ğŸ“Š çµè«–: é¡”æ¤œå‡ºç‰¹å¾´é‡ã¯äºˆæ¸¬æ€§èƒ½ã«{conclusion}")
print(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿è¦æ¨¡: {len(df_with_images):,}ä»¶ï¼ˆã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãªã—ï¼‰")
print(f"ğŸ“„ è©³ç´°çµæœ: full_dataset_face_analysis_results.json")
print(f"ğŸ“„ å®Œå…¨ãƒ‡ãƒ¼ã‚¿: youtube_complete_with_faces_full.csv")

print(f"\n" + "="*60)
print("å…¨ãƒ‡ãƒ¼ã‚¿åˆ†æå®Œäº†ï¼")
print("="*60)