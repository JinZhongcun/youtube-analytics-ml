#!/usr/bin/env python3
"""
é¡”æ¤œå‡ºç‰¹å¾´é‡ã‚’å«ã‚€ãƒ¢ãƒ‡ãƒ« vs å«ã¾ãªã„ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½æ¯”è¼ƒ
ç‰¹å¾´é‡é‡è¦åº¦ã®å¤‰åŒ–ã‚’å®šé‡çš„ã«åˆ†æ
"""
import pandas as pd
import numpy as np
import cv2
import os
import json
from sklearn.model_selection import cross_val_score, KFold, train_test_split
from sklearn.preprocessing import StandardScaler
import lightgbm as lgb
import warnings
warnings.filterwarnings('ignore')

print("="*60)
print("é¡”æ¤œå‡ºç‰¹å¾´é‡ã‚’å«ã‚€ãƒ¢ãƒ‡ãƒ« vs å«ã¾ãªã„ãƒ¢ãƒ‡ãƒ«ã®æ¯”è¼ƒ")
print("="*60)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv('youtube_top_new_complete.csv')
print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(df)}ä»¶")

# ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒã®å­˜åœ¨ç¢ºèª
thumbnails_dir = 'thumbnails'
df['thumbnail_exists'] = df['video_id'].apply(
    lambda x: os.path.exists(os.path.join(thumbnails_dir, f'{x}.jpg'))
)
print(f"ã‚µãƒ ãƒã‚¤ãƒ«ç”»åƒãŒå­˜åœ¨: {df['thumbnail_exists'].sum()}/{len(df)}ä»¶")

# ç”»åƒãŒå­˜åœ¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã¿ä½¿ç”¨
df_with_images = df[df['thumbnail_exists']].copy()
print(f"åˆ†æå¯¾è±¡: {len(df_with_images)}ä»¶")

def detect_face(video_id):
    """OpenCV Haar Cascadeã§é¡”æ¤œå‡º"""
    img_path = os.path.join(thumbnails_dir, f'{video_id}.jpg')
    
    try:
        img = cv2.imread(img_path)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # é¡”æ¤œå‡º
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)
        
        return int(len(faces) > 0)
    except Exception as e:
        return 0

# ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‚’åˆ¶é™ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ï¼‰
sample_size = min(1000, len(df_with_images))
df_sample = df_with_images.sample(n=sample_size, random_state=42).copy()
print(f"ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º: {sample_size}ä»¶ï¼ˆé«˜é€ŸåŒ–ã®ãŸã‚ï¼‰")

# é¡”æ¤œå‡ºå®Ÿè¡Œ
print(f"\n=== é¡”æ¤œå‡ºå®Ÿè¡Œä¸­ ===")
face_results = []
for i, video_id in enumerate(df_sample['video_id']):
    if i % 100 == 0:
        print(f"  {i}/{sample_size}ä»¶å‡¦ç†æ¸ˆã¿ ({i/sample_size*100:.1f}%)")
    
    has_face = detect_face(video_id)
    face_results.append(has_face)

df_sample['has_face'] = face_results

# åŸºæœ¬çµ±è¨ˆ
face_count = sum(face_results)
no_face_count = len(face_results) - face_count
print(f"é¡”ã‚ã‚Š: {face_count}ä»¶, é¡”ãªã—: {no_face_count}ä»¶")

# æ™‚é–“é–¢é€£ã®ç‰¹å¾´é‡ä½œæˆ
df_sample['published_at'] = pd.to_datetime(df_sample['published_at']).dt.tz_localize(None)
df_sample['hour_published'] = df_sample['published_at'].dt.hour
df_sample['days_since_publish'] = (pd.Timestamp.now() - df_sample['published_at']).dt.days

# ãƒ­ã‚°å¤‰æ›
df_sample['log_views'] = np.log10(df_sample['views'] + 1)
df_sample['log_subscribers'] = np.log1p(df_sample['subscribers'])

# åŸºæœ¬ç‰¹å¾´é‡ï¼ˆé¡”æ¤œå‡ºãªã—ï¼‰
basic_features = [
    'video_duration', 'tags_count', 'description_length',
    'object_complexity', 'element_complexity', 'brightness', 'colorfulness',
    'subscribers', 'log_subscribers', 'hour_published', 'days_since_publish'
]

# é¡”æ¤œå‡ºã‚’å«ã‚€ç‰¹å¾´é‡
face_features = basic_features + ['has_face']

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X_basic = df_sample[basic_features].fillna(0)
X_face = df_sample[face_features].fillna(0)
y = df_sample['log_views']

# äº¤å·®æ¤œè¨¼è¨­å®š
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

print(f"\n=== ãƒ¢ãƒ‡ãƒ«1: åŸºæœ¬ç‰¹å¾´é‡ã®ã¿ï¼ˆ{len(basic_features)}ç‰¹å¾´é‡ï¼‰===")
lgb_basic = lgb.LGBMRegressor(
    num_leaves=31, max_depth=6, min_child_samples=30,
    lambda_l2=0.1, feature_fraction=0.8, bagging_fraction=0.8,
    learning_rate=0.05, n_estimators=200, random_state=42, verbosity=-1
)

cv_scores_basic = cross_val_score(lgb_basic, X_basic, y, cv=kfold, scoring='r2')
print(f"CV RÂ²: {cv_scores_basic.mean():.4f} Â± {cv_scores_basic.std():.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦å–å¾—
lgb_basic.fit(X_basic, y)
importance_basic = pd.DataFrame({
    'feature': basic_features,
    'importance': lgb_basic.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\nåŸºæœ¬ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆTop 8ï¼‰:")
total_importance_basic = importance_basic['importance'].sum()
for _, row in importance_basic.head(8).iterrows():
    pct = (row['importance'] / total_importance_basic) * 100
    print(f"  {row['feature']:<20} {row['importance']:>6.0f} ({pct:>5.1f}%)")

print(f"\n=== ãƒ¢ãƒ‡ãƒ«2: é¡”æ¤œå‡ºç‰¹å¾´é‡å«ã‚€ï¼ˆ{len(face_features)}ç‰¹å¾´é‡ï¼‰===")
lgb_face = lgb.LGBMRegressor(
    num_leaves=31, max_depth=6, min_child_samples=30,
    lambda_l2=0.1, feature_fraction=0.8, bagging_fraction=0.8,
    learning_rate=0.05, n_estimators=200, random_state=42, verbosity=-1
)

cv_scores_face = cross_val_score(lgb_face, X_face, y, cv=kfold, scoring='r2')
print(f"CV RÂ²: {cv_scores_face.mean():.4f} Â± {cv_scores_face.std():.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦å–å¾—
lgb_face.fit(X_face, y)
importance_face = pd.DataFrame({
    'feature': face_features,
    'importance': lgb_face.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\né¡”æ¤œå‡ºå«ã‚€ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆTop 9ï¼‰:")
total_importance_face = importance_face['importance'].sum()
for _, row in importance_face.head(9).iterrows():
    pct = (row['importance'] / total_importance_face) * 100
    marker = " â† é¡”æ¤œå‡º" if row['feature'] == 'has_face' else ""
    print(f"  {row['feature']:<20} {row['importance']:>6.0f} ({pct:>5.1f}%){marker}")

# æ€§èƒ½å·®åˆ†æ
performance_diff = cv_scores_face.mean() - cv_scores_basic.mean()
print(f"\n=== æ€§èƒ½æ¯”è¼ƒ ===")
print(f"åŸºæœ¬ãƒ¢ãƒ‡ãƒ«:     RÂ² = {cv_scores_basic.mean():.4f} Â± {cv_scores_basic.std():.4f}")
print(f"é¡”æ¤œå‡ºå«ã‚€:     RÂ² = {cv_scores_face.mean():.4f} Â± {cv_scores_face.std():.4f}")
print(f"æ€§èƒ½å·®:         {performance_diff:+.4f}")
print(f"æ”¹å–„ç‡:         {(performance_diff/cv_scores_basic.mean())*100:+.2f}%")

# é¡”æ¤œå‡ºç‰¹å¾´é‡ã®é †ä½
has_face_rank = importance_face[importance_face['feature'] == 'has_face'].index[0] + 1
has_face_importance = importance_face[importance_face['feature'] == 'has_face']['importance'].iloc[0]
has_face_pct = (has_face_importance / total_importance_face) * 100

print(f"\n=== é¡”æ¤œå‡ºç‰¹å¾´é‡ã®åˆ†æ ===")
print(f"has_faceé †ä½:   {has_face_rank}ä½ / {len(face_features)}ç‰¹å¾´é‡")
print(f"é‡è¦åº¦:         {has_face_importance:.0f} ({has_face_pct:.1f}%)")

# çµæœä¿å­˜
results = {
    'dataset_info': {
        'original_data': len(df),
        'with_thumbnails': len(df_with_images), 
        'sample_size': sample_size,
        'face_count': int(face_count),
        'no_face_count': int(no_face_count)
    },
    'model_performance': {
        'basic_model': {
            'features_count': len(basic_features),
            'cv_r2_mean': float(cv_scores_basic.mean()),
            'cv_r2_std': float(cv_scores_basic.std())
        },
        'face_model': {
            'features_count': len(face_features),
            'cv_r2_mean': float(cv_scores_face.mean()),
            'cv_r2_std': float(cv_scores_face.std())
        },
        'comparison': {
            'performance_diff': float(performance_diff),
            'improvement_rate_pct': float((performance_diff/cv_scores_basic.mean())*100)
        }
    },
    'feature_importance': {
        'basic_model': importance_basic.head(8).to_dict('records'),
        'face_model': importance_face.head(9).to_dict('records')
    },
    'face_feature_analysis': {
        'rank': int(has_face_rank),
        'importance': float(has_face_importance),
        'percentage': float(has_face_pct)
    }
}

with open('face_feature_comparison_results.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"\n=== æœ€çµ‚çµè«– ===")
if performance_diff > 0:
    print(f"âœ… é¡”æ¤œå‡ºç‰¹å¾´é‡ã¯æ€§èƒ½ã‚’ {performance_diff:.4f} æ”¹å–„")
    print(f"âœ… {has_face_rank}ä½ã®é‡è¦åº¦ ({has_face_pct:.1f}%)")
    conclusion = "æœ‰åŠ¹"
else:
    print(f"âŒ é¡”æ¤œå‡ºç‰¹å¾´é‡ã¯æ€§èƒ½ã‚’ {performance_diff:.4f} æ‚ªåŒ–")
    print(f"âŒ {has_face_rank}ä½ã®é‡è¦åº¦ ({has_face_pct:.1f}%)")
    conclusion = "ç„¡åŠ¹"

print(f"ğŸ“Š çµè«–: é¡”æ¤œå‡ºç‰¹å¾´é‡ã¯äºˆæ¸¬æ€§èƒ½ã«{conclusion}")
print(f"ğŸ“„ è©³ç´°çµæœ: face_feature_comparison_results.json")