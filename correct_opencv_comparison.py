#!/usr/bin/env python3
"""
æ­£ã—ã„OpenCVç‰¹å¾´é‡æ¯”è¼ƒåˆ†æ
- OpenCVç‰¹å¾´é‡ãªã— vs OpenCVç‰¹å¾´é‡ã‚ã‚Šï¼ˆé¡”æ¤œå‡ºå«ã‚€ï¼‰ã®å®Œå…¨æ¯”è¼ƒ
- å…¨6,062ä»¶ãƒ‡ãƒ¼ã‚¿ã§ã®åˆ†æ
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, KFold
import lightgbm as lgb
import json

print("="*70)
print("æ­£ã—ã„OpenCVç‰¹å¾´é‡æ¯”è¼ƒåˆ†æ - å…¨6,062ä»¶ãƒ‡ãƒ¼ã‚¿")
print("="*70)

# å…ƒãƒ‡ãƒ¼ã‚¿ï¼ˆ6,062ä»¶ï¼‰èª­ã¿è¾¼ã¿
df_original = pd.read_csv('youtube_top_new_complete.csv')
print(f"å…ƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(df_original)}ä»¶")

# é¡”æ¤œå‡ºãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df_faces = pd.read_csv('youtube_complete_with_faces_full.csv')
print(f"é¡”æ¤œå‡ºãƒ‡ãƒ¼ã‚¿: {len(df_faces)}ä»¶")

# has_faceã‚«ãƒ©ãƒ ã‚’å…ƒãƒ‡ãƒ¼ã‚¿ã«ãƒãƒ¼ã‚¸ï¼ˆãªã„å ´åˆã¯0ï¼‰
df_full = df_original.merge(
    df_faces[['video_id', 'has_face']], 
    on='video_id', 
    how='left'
)

# has_faceãŒNaNã®å ´åˆã¯0ï¼ˆé¡”ãªã—ï¼‰ã¨ã—ã¦æ‰±ã†
df_full['has_face'] = df_full['has_face'].fillna(0).astype(int)

print(f"ãƒãƒ¼ã‚¸å¾Œãƒ‡ãƒ¼ã‚¿: {len(df_full)}ä»¶")

# æ™‚é–“é–¢é€£ç‰¹å¾´é‡ã®ä½œæˆ
df_full['published_at'] = pd.to_datetime(df_full['published_at']).dt.tz_localize(None)
df_full['hour_published'] = df_full['published_at'].dt.hour
df_full['days_since_publish'] = (pd.Timestamp.now() - df_full['published_at']).dt.days

# ãƒ­ã‚°å¤‰æ›
df_full['log_views'] = np.log10(df_full['views'] + 1)
df_full['log_subscribers'] = np.log1p(df_full['subscribers'])

# OpenCVç‰¹å¾´é‡ãªã—ï¼ˆåŸºæœ¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰
basic_features = [
    'video_duration', 'tags_count', 'description_length',
    'subscribers', 'log_subscribers', 'hour_published', 'days_since_publish'
]

# OpenCVç‰¹å¾´é‡ã‚ã‚Šï¼ˆç”»åƒè§£æç‰¹å¾´é‡ã‚’å…¨ã¦å«ã‚€ï¼‰
opencv_features = basic_features + [
    'object_complexity', 'element_complexity', 'brightness', 'colorfulness', 'has_face'
]

print(f"\nğŸ” ç‰¹å¾´é‡æ§‹æˆ:")
print(f"  åŸºæœ¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿: {len(basic_features)}å€‹")
print(f"    {basic_features}")
print(f"  OpenCVç”»åƒè§£æå«ã‚€: {len(opencv_features)}å€‹")
print(f"    OpenCVç‰¹å¾´é‡: object_complexity, element_complexity, brightness, colorfulness, has_face")

# OpenCVç‰¹å¾´é‡ã®åˆ†å¸ƒç¢ºèª
opencv_only = ['object_complexity', 'element_complexity', 'brightness', 'colorfulness', 'has_face']
print(f"\nğŸ“Š OpenCVç‰¹å¾´é‡ã®åŸºæœ¬çµ±è¨ˆ:")
for feature in opencv_only:
    if feature in df_full.columns:
        values = df_full[feature].dropna()
        print(f"  {feature}: å¹³å‡={values.mean():.2f}, æ¨™æº–åå·®={values.std():.2f}")

# é¡”æ¤œå‡ºã®åˆ†å¸ƒ
face_count = df_full['has_face'].sum()
no_face_count = len(df_full) - face_count
print(f"\nğŸ‘¤ é¡”æ¤œå‡ºåˆ†å¸ƒ:")
print(f"  é¡”ã‚ã‚Š: {face_count}ä»¶ ({face_count/len(df_full)*100:.1f}%)")
print(f"  é¡”ãªã—: {no_face_count}ä»¶ ({no_face_count/len(df_full)*100:.1f}%)")

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X_basic = df_full[basic_features].fillna(0)
X_opencv = df_full[opencv_features].fillna(0)
y = df_full['log_views']

print(f"\nğŸ“‹ åˆ†æè¨­å®š:")
print(f"  ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X_basic)}ä»¶")
print(f"  ç›®çš„å¤‰æ•°: log_views")
print(f"  äº¤å·®æ¤œè¨¼: 5-fold CV")

# äº¤å·®æ¤œè¨¼è¨­å®š
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

print(f"\n=== ãƒ¢ãƒ‡ãƒ«1: åŸºæœ¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼ˆ{len(basic_features)}ç‰¹å¾´é‡ï¼‰===")
lgb_basic = lgb.LGBMRegressor(
    num_leaves=31, max_depth=6, min_child_samples=30,
    lambda_l2=0.1, feature_fraction=0.8, bagging_fraction=0.8,
    learning_rate=0.05, n_estimators=200, random_state=42, verbosity=-1
)

print("äº¤å·®æ¤œè¨¼å®Ÿè¡Œä¸­...")
cv_scores_basic = cross_val_score(lgb_basic, X_basic, y, cv=kfold, scoring='r2')
print(f"CV RÂ²: {cv_scores_basic.mean():.4f} Â± {cv_scores_basic.std():.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦å–å¾—
lgb_basic.fit(X_basic, y)
importance_basic = pd.DataFrame({
    'feature': basic_features,
    'importance': lgb_basic.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\nåŸºæœ¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç‰¹å¾´é‡ã®é‡è¦åº¦:")
total_importance_basic = importance_basic['importance'].sum()
for i, (_, row) in enumerate(importance_basic.iterrows()):
    pct = (row['importance'] / total_importance_basic) * 100
    print(f"  {i+1:>2}. {row['feature']:<20} {row['importance']:>6.0f} ({pct:>5.1f}%)")

print(f"\n=== ãƒ¢ãƒ‡ãƒ«2: OpenCVç”»åƒè§£æç‰¹å¾´é‡å«ã‚€ï¼ˆ{len(opencv_features)}ç‰¹å¾´é‡ï¼‰===")
lgb_opencv = lgb.LGBMRegressor(
    num_leaves=31, max_depth=6, min_child_samples=30,
    lambda_l2=0.1, feature_fraction=0.8, bagging_fraction=0.8,
    learning_rate=0.05, n_estimators=200, random_state=42, verbosity=-1
)

print("äº¤å·®æ¤œè¨¼å®Ÿè¡Œä¸­...")
cv_scores_opencv = cross_val_score(lgb_opencv, X_opencv, y, cv=kfold, scoring='r2')
print(f"CV RÂ²: {cv_scores_opencv.mean():.4f} Â± {cv_scores_opencv.std():.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦å–å¾—
lgb_opencv.fit(X_opencv, y)
importance_opencv = pd.DataFrame({
    'feature': opencv_features,
    'importance': lgb_opencv.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\nOpenCVå«ã‚€å…¨ç‰¹å¾´é‡ã®é‡è¦åº¦:")
total_importance_opencv = importance_opencv['importance'].sum()
for i, (_, row) in enumerate(importance_opencv.iterrows()):
    pct = (row['importance'] / total_importance_opencv) * 100
    
    # OpenCVç‰¹å¾´é‡ã‚’ãƒãƒ¼ã‚¯
    if row['feature'] in ['object_complexity', 'element_complexity', 'brightness', 'colorfulness']:
        marker = " â† ğŸ“·OpenCVç”»åƒ"
    elif row['feature'] == 'has_face':
        marker = " â† ğŸ‘¤OpenCVé¡”"
    else:
        marker = " â† ğŸ“ŠåŸºæœ¬ãƒ‡ãƒ¼ã‚¿"
    
    print(f"  {i+1:>2}. {row['feature']:<20} {row['importance']:>6.0f} ({pct:>5.1f}%){marker}")

# æ€§èƒ½å·®åˆ†æ
performance_diff = cv_scores_opencv.mean() - cv_scores_basic.mean()

print(f"\n=== æœ€çµ‚æ€§èƒ½æ¯”è¼ƒï¼ˆå…¨{len(df_full)}ä»¶ãƒ‡ãƒ¼ã‚¿ï¼‰===")
print(f"åŸºæœ¬ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ã¿: RÂ² = {cv_scores_basic.mean():.4f} Â± {cv_scores_basic.std():.4f}")
print(f"OpenCVç‰¹å¾´é‡å«ã‚€:   RÂ² = {cv_scores_opencv.mean():.4f} Â± {cv_scores_opencv.std():.4f}")
print(f"æ€§èƒ½å·®:             {performance_diff:+.4f}")
print(f"æ”¹å–„ç‡:             {(performance_diff/cv_scores_basic.mean())*100:+.2f}%")

# OpenCVç‰¹å¾´é‡ã®è²¢çŒ®åº¦åˆ†æ
opencv_importance_sum = 0
for feature in ['object_complexity', 'element_complexity', 'brightness', 'colorfulness', 'has_face']:
    feature_row = importance_opencv[importance_opencv['feature'] == feature]
    if not feature_row.empty:
        opencv_importance_sum += feature_row['importance'].iloc[0]

opencv_contribution_pct = (opencv_importance_sum / total_importance_opencv) * 100

print(f"\n=== OpenCVç‰¹å¾´é‡ã®è©³ç´°åˆ†æ ===")
print(f"OpenCVç‰¹å¾´é‡ã®åˆè¨ˆé‡è¦åº¦: {opencv_importance_sum:.0f} ({opencv_contribution_pct:.1f}%)")

opencv_rankings = []
for feature in ['object_complexity', 'element_complexity', 'brightness', 'colorfulness', 'has_face']:
    feature_row = importance_opencv[importance_opencv['feature'] == feature]
    if not feature_row.empty:
        rank = importance_opencv[importance_opencv['feature'] == feature].index[0] + 1
        importance = feature_row['importance'].iloc[0]
        pct = (importance / total_importance_opencv) * 100
        opencv_rankings.append((feature, rank, importance, pct))

print(f"\nOpenCVå„ç‰¹å¾´é‡ã®é †ä½:")
for feature, rank, importance, pct in sorted(opencv_rankings, key=lambda x: x[1]):
    print(f"  {rank:>2}ä½. {feature:<20} {importance:>6.0f} ({pct:>5.1f}%)")

# çµ±è¨ˆçš„æœ‰æ„æ€§ã®ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
improvement_significant = abs(performance_diff) > cv_scores_basic.std()
print(f"\nçµ±è¨ˆçš„æ„å‘³: {'æœ‰æ„ãªæ”¹å–„' if improvement_significant and performance_diff > 0 else 'èª¤å·®ç¯„å›²å†…'}")

# çµæœä¿å­˜ç”¨ãƒ‡ãƒ¼ã‚¿
results = {
    "analysis_type": "OPENCV_FEATURES_COMPARISON",
    "dataset_info": {
        "total_samples": len(df_full),
        "face_count": int(face_count),
        "no_face_count": int(no_face_count),
        "face_ratio": float(face_count/len(df_full))
    },
    "model_performance": {
        "basic_metadata_only": {
            "cv_r2_mean": float(cv_scores_basic.mean()),
            "cv_r2_std": float(cv_scores_basic.std()),
            "features_count": len(basic_features),
            "features": basic_features
        },
        "opencv_features_included": {
            "cv_r2_mean": float(cv_scores_opencv.mean()),
            "cv_r2_std": float(cv_scores_opencv.std()),
            "features_count": len(opencv_features),
            "opencv_features": ['object_complexity', 'element_complexity', 'brightness', 'colorfulness', 'has_face']
        },
        "comparison": {
            "performance_diff": float(performance_diff),
            "improvement_pct": float((performance_diff/cv_scores_basic.mean())*100),
            "is_significant": bool(improvement_significant and performance_diff > 0)
        }
    },
    "opencv_analysis": {
        "total_contribution_pct": float(opencv_contribution_pct),
        "individual_rankings": [
            {
                "feature": feature,
                "rank": int(rank),
                "importance": float(importance), 
                "percentage": float(pct)
            }
            for feature, rank, importance, pct in opencv_rankings
        ]
    }
}

# JSONä¿å­˜ï¼ˆboolã‚¨ãƒ©ãƒ¼å›é¿ï¼‰
with open('correct_opencv_comparison_results.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"\n=== æœ€çµ‚çµè«– ===")
if performance_diff > 0 and improvement_significant:
    print(f"âœ… OpenCVç”»åƒè§£æç‰¹å¾´é‡ã¯æ€§èƒ½ã‚’ {performance_diff:.4f} æœ‰æ„ã«æ”¹å–„")
    conclusion = "éå¸¸ã«æœ‰åŠ¹"
elif performance_diff > 0:
    print(f"ğŸ“Š OpenCVç”»åƒè§£æç‰¹å¾´é‡ã¯æ€§èƒ½ã‚’ {performance_diff:.4f} æ”¹å–„ï¼ˆèª¤å·®ç¯„å›²å†…ï¼‰")
    conclusion = "ã‚„ã‚„æœ‰åŠ¹"
else:
    print(f"âŒ OpenCVç”»åƒè§£æç‰¹å¾´é‡ã¯æ€§èƒ½ã‚’æ‚ªåŒ–")
    conclusion = "ç„¡åŠ¹"

print(f"ğŸ“Š OpenCVç‰¹å¾´é‡åˆè¨ˆè²¢çŒ®åº¦: {opencv_contribution_pct:.1f}%")
print(f"ğŸ“Š çµè«–: OpenCVç”»åƒè§£æç‰¹å¾´é‡ã¯{conclusion}")
print(f"ğŸ“„ è©³ç´°çµæœ: correct_opencv_comparison_results.json")

print(f"\n" + "="*70)
print("æ­£ã—ã„OpenCVç‰¹å¾´é‡æ¯”è¼ƒåˆ†æå®Œäº†ï¼")
print("="*70)