#!/usr/bin/env python3
"""
æ—¢å­˜CSVã®has_faceã‚«ãƒ©ãƒ ã‚’ä½¿ã£ã¦å˜ç´”æ¯”è¼ƒ
å…¨4,817ä»¶ã§ã®é¡”æ¤œå‡ºç‰¹å¾´é‡ã®åŠ¹æœåˆ†æ
"""
import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score, KFold
import lightgbm as lgb
import json

print("="*60)
print("has_faceã‚«ãƒ©ãƒ ä½¿ç”¨ã§ã®å…¨ãƒ‡ãƒ¼ã‚¿æ¯”è¼ƒåˆ†æ")
print("="*60)

# æ—¢å­˜ã®é¡”æ¤œå‡ºãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
df = pd.read_csv('youtube_complete_with_faces_full.csv')
print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ: {len(df)}ä»¶")

# has_faceã‚«ãƒ©ãƒ ã®çŠ¶æ³ç¢ºèª
face_count = df['has_face'].sum()
no_face_count = len(df) - face_count
print(f"é¡”ã‚ã‚Š: {face_count}ä»¶ ({face_count/len(df)*100:.1f}%)")
print(f"é¡”ãªã—: {no_face_count}ä»¶ ({no_face_count/len(df)*100:.1f}%)")

# åŸºæœ¬ç‰¹å¾´é‡ï¼ˆé¡”æ¤œå‡ºãªã—ï¼‰
basic_features = [
    'video_duration', 'tags_count', 'description_length',
    'object_complexity', 'element_complexity', 'brightness', 'colorfulness',
    'subscribers', 'log_subscribers', 'hour_published', 'days_since_publish'
]

# é¡”æ¤œå‡ºã‚’å«ã‚€ç‰¹å¾´é‡
face_features = basic_features + ['has_face']

# ãƒ‡ãƒ¼ã‚¿æº–å‚™
X_basic = df[basic_features].fillna(0)
X_face = df[face_features].fillna(0)
y = df['log_views']

print(f"\nç‰¹å¾´é‡æº–å‚™:")
print(f"  åŸºæœ¬ç‰¹å¾´é‡: {len(basic_features)}å€‹")
print(f"  é¡”æ¤œå‡ºå«ã‚€: {len(face_features)}å€‹")
print(f"  å…¨ã‚µãƒ³ãƒ—ãƒ«æ•°: {len(X_basic)}ä»¶")

# äº¤å·®æ¤œè¨¼è¨­å®š
kfold = KFold(n_splits=5, shuffle=True, random_state=42)

print(f"\n=== ãƒ¢ãƒ‡ãƒ«1: åŸºæœ¬ç‰¹å¾´é‡ã®ã¿ï¼ˆ{len(basic_features)}ç‰¹å¾´é‡ï¼‰===")
lgb_basic = lgb.LGBMRegressor(
    num_leaves=31, max_depth=6, min_child_samples=30,
    lambda_l2=0.1, feature_fraction=0.8, bagging_fraction=0.8,
    learning_rate=0.05, n_estimators=200, random_state=42, verbosity=-1
)

cv_scores_basic = cross_val_score(lgb_basic, X_basic, y, cv=kfold, scoring='r2')
print(f"CV RÂ²: {cv_scores_basic.mean():.4f} Â± {cv_scores_basic.std():.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦å–å¾—
lgb_basic.fit(X_basic, y)
importance_basic = pd.DataFrame({
    'feature': basic_features,
    'importance': lgb_basic.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\nåŸºæœ¬ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆTop 10ï¼‰:")
total_importance_basic = importance_basic['importance'].sum()
for _, row in importance_basic.head(10).iterrows():
    pct = (row['importance'] / total_importance_basic) * 100
    print(f"  {row['feature']:<20} {row['importance']:>6.0f} ({pct:>5.1f}%)")

print(f"\n=== ãƒ¢ãƒ‡ãƒ«2: é¡”æ¤œå‡ºç‰¹å¾´é‡å«ã‚€ï¼ˆ{len(face_features)}ç‰¹å¾´é‡ï¼‰===")
lgb_face = lgb.LGBMRegressor(
    num_leaves=31, max_depth=6, min_child_samples=30,
    lambda_l2=0.1, feature_fraction=0.8, bagging_fraction=0.8,
    learning_rate=0.05, n_estimators=200, random_state=42, verbosity=-1
)

cv_scores_face = cross_val_score(lgb_face, X_face, y, cv=kfold, scoring='r2')
print(f"CV RÂ²: {cv_scores_face.mean():.4f} Â± {cv_scores_face.std():.4f}")

# ç‰¹å¾´é‡é‡è¦åº¦å–å¾—
lgb_face.fit(X_face, y)
importance_face = pd.DataFrame({
    'feature': face_features,
    'importance': lgb_face.feature_importances_
}).sort_values('importance', ascending=False)

print(f"\né¡”æ¤œå‡ºå«ã‚€ç‰¹å¾´é‡ã®é‡è¦åº¦ï¼ˆå…¨{len(face_features)}ç‰¹å¾´é‡ï¼‰:")
total_importance_face = importance_face['importance'].sum()
for i, (_, row) in enumerate(importance_face.iterrows()):
    pct = (row['importance'] / total_importance_face) * 100
    marker = " â† ğŸ‘¤é¡”æ¤œå‡º" if row['feature'] == 'has_face' else ""
    rank = i + 1
    print(f"  {rank:>2}. {row['feature']:<18} {row['importance']:>6.0f} ({pct:>5.1f}%){marker}")

# æ€§èƒ½å·®åˆ†æ
performance_diff = cv_scores_face.mean() - cv_scores_basic.mean()

print(f"\n=== æœ€çµ‚æ€§èƒ½æ¯”è¼ƒï¼ˆå…¨{len(df)}ä»¶ãƒ‡ãƒ¼ã‚¿ï¼‰===")
print(f"åŸºæœ¬ãƒ¢ãƒ‡ãƒ«:     RÂ² = {cv_scores_basic.mean():.4f} Â± {cv_scores_basic.std():.4f}")
print(f"é¡”æ¤œå‡ºå«ã‚€:     RÂ² = {cv_scores_face.mean():.4f} Â± {cv_scores_face.std():.4f}")
print(f"æ€§èƒ½å·®:         {performance_diff:+.4f}")
print(f"æ”¹å–„ç‡:         {(performance_diff/cv_scores_basic.mean())*100:+.2f}%")

# é¡”æ¤œå‡ºç‰¹å¾´é‡ã®é †ä½åˆ†æ
has_face_row = importance_face[importance_face['feature'] == 'has_face']
has_face_rank = has_face_row.index[0] + 1
has_face_importance = has_face_row['importance'].iloc[0]
has_face_pct = (has_face_importance / total_importance_face) * 100

print(f"\n=== é¡”æ¤œå‡ºç‰¹å¾´é‡ã®è©³ç´°åˆ†æ ===")
print(f"has_faceé †ä½:   {has_face_rank}ä½ / {len(face_features)}ç‰¹å¾´é‡")
print(f"é‡è¦åº¦:         {has_face_importance:.0f} ({has_face_pct:.2f}%)")
print(f"ãƒ‡ãƒ¼ã‚¿è¦æ¨¡:     {len(df):,}ä»¶ï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãªã—ï¼‰")

# çµ±è¨ˆçš„æœ‰æ„æ€§ã®ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
improvement_significant = abs(performance_diff) > cv_scores_basic.std()
print(f"çµ±è¨ˆçš„æ„å‘³:     {'æœ‰æ„' if improvement_significant else 'èª¤å·®ç¯„å›²å†…'}")

# çµæœä¿å­˜
results = {
    "dataset_info": {
        "total_samples": len(df),
        "face_count": int(face_count),
        "no_face_count": int(no_face_count),
        "face_ratio": float(face_count/len(df))
    },
    "model_performance": {
        "basic_model": {
            "cv_r2_mean": float(cv_scores_basic.mean()),
            "cv_r2_std": float(cv_scores_basic.std()),
            "features_count": len(basic_features)
        },
        "face_model": {
            "cv_r2_mean": float(cv_scores_face.mean()),
            "cv_r2_std": float(cv_scores_face.std()),
            "features_count": len(face_features)
        },
        "comparison": {
            "performance_diff": float(performance_diff),
            "improvement_pct": float((performance_diff/cv_scores_basic.mean())*100),
            "is_significant": improvement_significant
        }
    },
    "feature_importance": {
        "basic_model": [
            {
                "rank": i+1,
                "feature": row['feature'],
                "importance": float(row['importance']),
                "percentage": float((row['importance'] / total_importance_basic) * 100)
            }
            for i, (_, row) in enumerate(importance_basic.iterrows())
        ],
        "face_model": [
            {
                "rank": i+1,
                "feature": row['feature'],
                "importance": float(row['importance']),
                "percentage": float((row['importance'] / total_importance_face) * 100)
            }
            for i, (_, row) in enumerate(importance_face.iterrows())
        ]
    },
    "face_analysis": {
        "rank": int(has_face_rank),
        "importance": float(has_face_importance),
        "percentage": float(has_face_pct),
        "conclusion": "EFFECTIVE" if performance_diff > 0 and improvement_significant else "MINIMAL_IMPACT"
    }
}

with open('final_face_comparison_results.json', 'w', encoding='utf-8') as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"\n=== æœ€çµ‚çµè«– ===")
if performance_diff > 0 and improvement_significant:
    print(f"âœ… é¡”æ¤œå‡ºç‰¹å¾´é‡ã¯æ€§èƒ½ã‚’ {performance_diff:.4f} æœ‰æ„ã«æ”¹å–„")
    conclusion = "æœ‰åŠ¹"
else:
    print(f"ğŸ“Š é¡”æ¤œå‡ºç‰¹å¾´é‡ã®åŠ¹æœã¯å¾®å°ã¾ãŸã¯èª¤å·®ç¯„å›²å†…")
    conclusion = "å®Ÿè³ªçš„ã«ç„¡åŠ¹"

print(f"ğŸ“Š é †ä½: {has_face_rank}ä½ / {len(face_features)}ç‰¹å¾´é‡")
print(f"ğŸ“Š é‡è¦åº¦: {has_face_pct:.2f}%")
print(f"ğŸ“Š çµè«–: é¡”æ¤œå‡ºç‰¹å¾´é‡ã¯{conclusion}")
print(f"ğŸ“„ è©³ç´°: final_face_comparison_results.json")

print(f"\n" + "="*60)
print("å…¨ãƒ‡ãƒ¼ã‚¿åˆ†æå®Œäº†ï¼")
print("="*60)